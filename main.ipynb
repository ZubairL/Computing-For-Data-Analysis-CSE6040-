{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "notebook_header"
    ]
   },
   "source": [
    "# Association rule mining\n",
    "\n",
    "In this notebook, you'll implement the basic pairwise association rule mining algorithm.\n",
    "\n",
    "To keep the implementation simple, you will apply your implementation to a simplified dataset, namely, letters (\"items\") in words (\"receipts\" or \"baskets\"). Having finished that code, you will then apply that code to some grocery store market basket data. If you write the code well, it will not be difficult to reuse building blocks from the letter case in the basket data case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Python version ===\n",
      "3.8.7 (default, Jan 25 2021, 11:14:52) \n",
      "[GCC 5.5.0 20171010]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"=== Python version ===\\n{sys.version}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "topic_intro"
    ]
   },
   "source": [
    "## Problem definition\n",
    "\n",
    "Let's say you have a fragment of text in some language. You wish to know whether there are association rules among the letters that appear in a word. In this problem:\n",
    "\n",
    "- Words are \"receipts\"\n",
    "- Letters within a word are \"items\"\n",
    "\n",
    "You want to know whether there are _association rules_ of the form, $a \\implies b$, where $a$ and $b$ are letters. You will write code to do that by calculating for each rule its _confidence_, $\\mathrm{conf}(a \\implies b)$. \"Confidence\" will be another name for an estimate of the conditional probability of $b$ given $a$, or $\\mathrm{Pr}[b \\,|\\, a]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample text input\n",
    "\n",
    "Let's carry out this analysis on a \"dummy\" text fragment, which graphic designers refer to as the [_lorem ipsum_](https://en.wikipedia.org/wiki/Lorem_ipsum):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "global_imports"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 characters:\n",
      "  \n",
      "Sed ut perspiciatis, unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium,  ...\n"
     ]
    }
   ],
   "source": [
    "latin_text = \"\"\"\n",
    "Sed ut perspiciatis, unde omnis iste natus error sit\n",
    "voluptatem accusantium doloremque laudantium, totam\n",
    "rem aperiam eaque ipsa, quae ab illo inventore\n",
    "veritatis et quasi architecto beatae vitae dicta\n",
    "sunt, explicabo. Nemo enim ipsam voluptatem, quia\n",
    "voluptas sit, aspernatur aut odit aut fugit, sed\n",
    "quia consequuntur magni dolores eos, qui ratione\n",
    "voluptatem sequi nesciunt, neque porro quisquam est,\n",
    "qui dolorem ipsum, quia dolor sit amet consectetur\n",
    "adipisci[ng] velit, sed quia non numquam [do] eius\n",
    "modi tempora inci[di]dunt, ut labore et dolore\n",
    "magnam aliquam quaerat voluptatem. Ut enim ad minima\n",
    "veniam, quis nostrum exercitationem ullam corporis\n",
    "suscipit laboriosam, nisi ut aliquid ex ea commodi\n",
    "consequatur? Quis autem vel eum iure reprehenderit,\n",
    "qui in ea voluptate velit esse, quam nihil molestiae\n",
    "consequatur, vel illum, qui dolorem eum fugiat, quo\n",
    "voluptas nulla pariatur?\n",
    "\n",
    "At vero eos et accusamus et iusto odio dignissimos\n",
    "ducimus, qui blanditiis praesentium voluptatum\n",
    "deleniti atque corrupti, quos dolores et quas\n",
    "molestias excepturi sint, obcaecati cupiditate non\n",
    "provident, similique sunt in culpa, qui officia\n",
    "deserunt mollitia animi, id est laborum et dolorum\n",
    "fuga. Et harum quidem rerum facilis est et expedita\n",
    "distinctio. Nam libero tempore, cum soluta nobis est\n",
    "eligendi optio, cumque nihil impedit, quo minus id,\n",
    "quod maxime placeat, facere possimus, omnis voluptas\n",
    "assumenda est, omnis dolor repellendus. Temporibus\n",
    "autem quibusdam et aut officiis debitis aut rerum\n",
    "necessitatibus saepe eveniet, ut et voluptates\n",
    "repudiandae sint et molestiae non recusandae. Itaque\n",
    "earum rerum hic tenetur a sapiente delectus, ut aut\n",
    "reiciendis voluptatibus maiores alias consequatur\n",
    "aut perferendis doloribus asperiores repellat.\n",
    "\"\"\"\n",
    "\n",
    "print(\"First 100 characters:\\n  {} ...\".format(latin_text[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 0** (ungraded). Look up and read the translation of _lorem ipsum_!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "exercise_text"
    ]
   },
   "source": [
    "**Data cleaning.** Like most data in the real world, this dataset is noisy. It has both uppercase and lowercase letters, words have repeated letters, and there are all sorts of non-alphabetic characters. For our analysis, we should keep all the letters and spaces (so we can identify distinct words), but we should ignore case and ignore repetition within a word.\n",
    "\n",
    "For example, the eighth word of this text is \"error.\" As an _itemset_, it consists of the three unique letters, $\\{e, o, r\\}$. That is, treat the word as a set, meaning you only keep the unique letters.\n",
    "\n",
    "This itemset has three possible _itempairs_: $\\{e, o\\}$, $\\{e, r\\}$, and $\\{o, r\\}$.\n",
    "\n",
    "> Since sets are unordered, note that we would regard $\\{e, o\\} = \\{o, e\\}$, which is why we say there are only three itempairs, rather than six.\n",
    "\n",
    "Start by writing some code to help \"clean up\" the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "exercise_text"
    ]
   },
   "source": [
    "**Exercise 1** (2 points). Complete the following function, `normalize_string(s)`. The input `s` is a string (`str` object). The function should return a new string with (a) all characters converted to lowercase and (b) all non-alphabetic, non-whitespace characters removed.\n",
    "\n",
    "> _Clarification_. Scanning the sample text, `latin_text`, you may see things that look like special cases. For instance, `inci[di]dunt` and `[do]`. For these, simply remove the non-alphabetic characters and only separate the words if there is explicit whitespace.\n",
    ">\n",
    "> For instance, `inci[di]dunt` would become `incididunt` (as a single word) and `[do]` would become `do` as a standalone word because the original string has whitespace on either side. A period or comma without whitespace would, similarly, just be treated as a non-alphabetic character inside a word _unless_ there is explicit whitespace. So `e pluribus.unum basium` would become `e pluribusunum basium` even though your common-sense understanding might separate `pluribus` and `unum`.\n",
    ">\n",
    "> _Hint_. Regard as a whitespace character anything \"whitespace-like.\" That is, consider not just regular spaces, but also tabs, newlines, and perhaps others. To detect whitespaces easily, look for a \"high-level\" function that can help you do so rather than checking for literal space characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_data"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sed ut perspiciatis, unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium, \n"
     ]
    }
   ],
   "source": [
    "### Define demo inputs\n",
    "demo_s_ex1 = latin_text[:100]\n",
    "print(demo_s_ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_output_md"
    ]
   },
   "source": [
    "<!-- Expected demo output text block -->\n",
    "The demo included in the solution cell below should display the following output:\n",
    "```\n",
    "sed ut perspiciatis unde omnis iste natus error sit\n",
    "voluptatem accusantium doloremque laudantium\n",
    "```\n",
    "<!-- Include any shout outs here -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "exercise_solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sed ut perspiciatis unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def normalize_string(s):\n",
    "    assert type (s) is str\n",
    "    clean = re.sub(r'[^\\w\\s]', '', s)\n",
    "    clean = clean.lower()\n",
    "    return clean\n",
    "    \n",
    "# Demo:\n",
    "print(normalize_string(demo_s_ex1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "test_data_boilerplate"
    ]
   },
   "source": [
    "<!-- Test Cell Boilerplate -->\n",
    "The cell below will test your solution for Exercise 1. The testing variables will be available for debugging under the following names in a dictionary format.\n",
    "- `input_vars` - Input variables for your solution. \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
    "- `returned_output_vars` - Outputs returned by your solution.\n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1",
     "locked": true,
     "points": "2",
     "solution": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### test_cell_ex1\n",
    "from tester_fw.testers import Tester\n",
    "\n",
    "conf = {\n",
    "    'case_file':'tc_1', \n",
    "    'func': normalize_string, # replace this with the function defined above\n",
    "    'inputs':{ # input config dict. keys are parameter names\n",
    "        's':{\n",
    "            'dtype':'str', # data type of param.\n",
    "            'check_modified':False,\n",
    "        }\n",
    "    },\n",
    "    'outputs':{\n",
    "        'output_0':{\n",
    "            'index':0,\n",
    "            'dtype':'str',\n",
    "            'check_dtype': True,\n",
    "            'check_col_dtypes': True, # Ignored if dtype is not df\n",
    "            'check_col_order': True, # Ignored if dtype is not df\n",
    "            'check_row_order': True, # Ignored if dtype is not df\n",
    "            'check_column_type': True, # Ignored if dtype is not df\n",
    "            'float_tolerance': 10 ** (-6)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "tester = Tester(conf, key=b's63L2lglDfBJpcKzxpwcyy61HyKnJNBOJXl9BMyWhyo=', path='resource/asnlib/publicdata/')\n",
    "for _ in range(70):\n",
    "    try:\n",
    "        tester.run_test()\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "    except:\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "        raise\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "exercise_text"
    ]
   },
   "source": [
    "**Exercise 2** (1 point). Implement the following function, `get_normalized_words(s)`. It takes as input a string `s` (i.e., a `str` object). It should normalize `s` and then return a list of its words. (That is, the function should not assume that the input `s` is normalized yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_data"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSed ut perspiciatis, unde omnis '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Define demo inputs\n",
    "\n",
    "demo_s_ex2 = latin_text[:33]\n",
    "demo_s_ex2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_output_md"
    ]
   },
   "source": [
    "<!-- Expected demo output text block -->\n",
    "The demo included in the solution cell below should display the following output:\n",
    "```\n",
    "['sed', 'ut', 'perspiciatis', 'unde', 'omnis']\n",
    "```\n",
    "<!-- Include any shout outs here -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "exercise_solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sed', 'ut', 'perspiciatis', 'unde', 'omnis']\n"
     ]
    }
   ],
   "source": [
    "def get_normalized_words (s):\n",
    "    assert type(s) is str\n",
    "    clean = re.sub(r'[^\\w\\s]', '', s)\n",
    "    clean = clean.lower()\n",
    "    clean = clean.split()\n",
    "    \n",
    "    return clean\n",
    "# Demo:\n",
    "print(get_normalized_words(demo_s_ex2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "test_data_boilerplate"
    ]
   },
   "source": [
    "<!-- Test Cell Boilerplate -->\n",
    "The cell below will test your solution for Exercise 2. The testing variables will be available for debugging under the following names in a dictionary format.\n",
    "- `input_vars` - Input variables for your solution. \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
    "- `returned_output_vars` - Outputs returned by your solution.\n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2",
     "locked": true,
     "points": "1",
     "solution": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### test_cell_ex2\n",
    "\n",
    "from tester_fw.testers import Tester\n",
    "\n",
    "conf = {\n",
    "    'case_file':'tc_2', \n",
    "    'func': get_normalized_words, # replace this with the function defined above\n",
    "    'inputs':{ # input config dict. keys are parameter names\n",
    "        's':{\n",
    "            'dtype':'str', # data type of param.\n",
    "            'check_modified':False,\n",
    "        }\n",
    "    },\n",
    "    'outputs':{\n",
    "        'output_0':{\n",
    "            'index':0,\n",
    "            'dtype':'',\n",
    "            'check_dtype': True,\n",
    "            'check_col_dtypes': True, # Ignored if dtype is not df\n",
    "            'check_col_order': True, # Ignored if dtype is not df\n",
    "            'check_row_order': True, # Ignored if dtype is not df\n",
    "            'check_column_type': True, # Ignored if dtype is not df\n",
    "            'float_tolerance': 10 ** (-6)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "tester = Tester(conf, key=b's63L2lglDfBJpcKzxpwcyy61HyKnJNBOJXl9BMyWhyo=', path='resource/asnlib/publicdata/')\n",
    "for _ in range(70):\n",
    "    try:\n",
    "        tester.run_test()\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "    except:\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "        raise\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "exercise_text"
    ]
   },
   "source": [
    "**Exercise 3** (2 points). Implement a function, `make_itemsets_unstructured_text(text)`. The input, `text`, is a string of unstructured text (like the `latin_text` example above). Your function should get the normalized words from the text, convert the characters of each word into an itemset and then return the list of all itemsets. These output itemsets should appear in the same order as their corresponding words in the input. You may find it helpful to call `get_normalized_words` in your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_data"
    ]
   },
   "outputs": [],
   "source": [
    "### Define demo inputs\n",
    "demo_text_ex3 = 'sed \\tut, perspiciatis\\n und.e omnis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_output_md"
    ]
   },
   "source": [
    "<!-- Expected demo output text block -->\n",
    "The demo included in the solution cell below should display the following output:\n",
    "```\n",
    "[{'d', 'e', 's'},\n",
    " {'t', 'u'},\n",
    " {'a', 'c', 'e', 'i', 'p', 'r', 's', 't'},\n",
    " {'d', 'e', 'n', 'u'},\n",
    " {'i', 'm', 'n', 'o', 's'}]\n",
    "```\n",
    "<!-- Include any shout outs here -->\n",
    "> Because sets are unordered, different versions of Python may produce sets with whose element-ordering differs from what you see above. However, the sets themselves should be in this order in the output list, since that is the order in which the corresponding words were given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "exercise_solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'d', 'e', 's'},\n",
       " {'t', 'u'},\n",
       " {'a', 'c', 'e', 'i', 'p', 'r', 's', 't'},\n",
       " {'d', 'e', 'n', 'u'},\n",
       " {'i', 'm', 'n', 'o', 's'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def make_itemsets_unstructured_text(text):\n",
    "    clean = re.sub(r'[^\\w\\s]', '', text)\n",
    "    clean = clean.lower()\n",
    "    clean = clean.split()\n",
    "    letter_list = []\n",
    "    for word in clean:\n",
    "        word_set = set()\n",
    "        for letter in word:\n",
    "            word_set.add(letter)\n",
    "        letter_list.append(word_set)\n",
    "    return letter_list\n",
    "make_itemsets_unstructured_text(demo_text_ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "test_data_boilerplate"
    ]
   },
   "source": [
    "<!-- Test Cell Boilerplate -->\n",
    "The cell below will test your solution for Exercise 3. The testing variables will be available for debugging under the following names in a dictionary format.\n",
    "- `input_vars` - Input variables for your solution. \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
    "- `returned_output_vars` - Outputs returned by your solution.\n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex3",
     "locked": true,
     "points": "2",
     "solution": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### test_cell_ex3\n",
    "\n",
    "from tester_fw.testers import Tester\n",
    "\n",
    "conf = {\n",
    "    'case_file':'tc_3', \n",
    "    'func': make_itemsets_unstructured_text, # replace this with the function defined above\n",
    "    'inputs':{ # input config dict. keys are parameter names\n",
    "        'text':{\n",
    "            'dtype':'list', # data type of param.\n",
    "            'check_modified':True,\n",
    "        }\n",
    "    },\n",
    "    'outputs':{\n",
    "        'output_0':{\n",
    "            'index':0,\n",
    "            'dtype':'list',\n",
    "            'check_dtype': True,\n",
    "            'check_col_dtypes': True, # Ignored if dtype is not df\n",
    "            'check_col_order': True, # Ignored if dtype is not df\n",
    "            'check_row_order': True, # Ignored if dtype is not df\n",
    "            'check_column_type': True, # Ignored if dtype is not df\n",
    "            'float_tolerance': 10 ** (-6)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "tester = Tester(conf, key=b's63L2lglDfBJpcKzxpwcyy61HyKnJNBOJXl9BMyWhyo=', path='resource/asnlib/publicdata/')\n",
    "for _ in range(70):\n",
    "    try:\n",
    "        tester.run_test()\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "    except:\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "        raise\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the basic algorithm\n",
    "\n",
    "Recall the pseudocode for the algorithm that Rachel and Rich derived together:\n",
    "\n",
    "![FindAssocRules (pseudocode)](https://ndownloader.figshare.com/files/7635700?private_link=3c473609741895a5cc2c)\n",
    "\n",
    "In the following series of exercises, let's implement this method. We'll build it \"bottom-up,\" first defining small pieces and working our way toward the complete algorithm. This method allows us to test each piece before combining them.\n",
    "\n",
    "Observe that the bulk of the work in this procedure is just updating these tables, $T$ and $C$. So your biggest implementation decision is how to store those. A good choice is to use a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Default dictionaries\n",
    "\n",
    "Recall that the overall algorithm requires maintaining a table of item-pair (tuples) counts. It would be convenient to use a dictionary to store this table, where keys refer to item-pairs and the values are the counts.\n",
    "\n",
    "However, with Python's built-in dictionaries, you always to have to check whether a key exists before updating it. For example, consider this code fragment:\n",
    "\n",
    "```python\n",
    "D = {'existing-key': 5} # Dictionary with one key-value pair\n",
    "\n",
    "D['existing-key'] += 1 # == 6\n",
    "D['new-key'] += 1  # Error: 'new-key' does not exist!\n",
    "```\n",
    "\n",
    "The second attempt causes an error because `'new-key'` is not yet a member of the dictionary. So, a more correct approach would be to do the following:\n",
    "\n",
    "```python\n",
    "D = {'existing-key': 5} # Dictionary with one key-value pair\n",
    "\n",
    "if 'existing-key' not in D:\n",
    "    D['existing-key'] = 0\n",
    "D['existing-key'] += 1\n",
    "   \n",
    "if 'new-key' not in D:\n",
    "    D['new-key'] = 0\n",
    "D['new-key'] += 1\n",
    "```\n",
    "\n",
    "This pattern is so common that there is a special form of dictionary, called a _default dictionary_, which is available from the `collections` module: [`collections.defaultdict`](https://docs.python.org/3/library/collections.html?highlight=defaultdict#collections.defaultdict).\n",
    "\n",
    "When you create a default dictionary, you need to provide a \"factory\" function that the dictionary can use to create an initial value when the key does *not* exist. For instance, in the preceding example, when the key was not present the code creates a new key with the initial value of an integer zero (0). Indeed, this default value is the one you get when you call `int()` with no arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'existing-key': 6, 'new-key': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "D2 = defaultdict(int) # Empty dictionary\n",
    "\n",
    "D2['existing-key'] = 5 # Create one key-value pair\n",
    "\n",
    "D2['existing-key'] += 1 # Update\n",
    "D2['new-key'] += 1\n",
    "\n",
    "print(D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "exercise_text"
    ]
   },
   "source": [
    "**Exercise 4** (2 points). Start by implementing a function that enumerates all item-pairs within an itemset and updates, _in-place_, a table that tracks the counts of those item-pairs.\n",
    "\n",
    "The signature of this function is:\n",
    "\n",
    "```python\n",
    "   def update_pair_counts(pair_counts, itemset):\n",
    "       ...\n",
    "```\n",
    "\n",
    "where `pair_counts` is the table to update and `itemset` is the itemset from which you need to enumerate item-pairs. You may assume `pair_counts` is a default dictionary. Each key is a pair of items `(a, b)`, and each value is the count. You may assume all items in `itemset` are distinct, i.e., that you may treat it as you would any set-like collection. Since the function will modify `pair_counts`, it does not need to return an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_data"
    ]
   },
   "outputs": [],
   "source": [
    "### Define demo inputs\n",
    "demo_itemset_ex4 = {'f', 'r', 'o', 'g'}\n",
    "demo_pair_counts_ex4 = defaultdict(int)\n",
    "demo_pair_counts_ex4.update(\n",
    "            {('o', 'e'): 1,\n",
    "             ('e', 'o'): 1,\n",
    "             ('o', 'r'): 1,\n",
    "             ('r', 'o'): 1,\n",
    "             ('e', 'r'): 1,\n",
    "             ('r', 'e'): 1})\n",
    "\n",
    "# This wrapper will return the updated pair_counts as a new object without modifying the original. \n",
    "# It's convenient for testing!\n",
    "def update_pair_counts_wrapper(pair_counts, itemset):\n",
    "    from copy import deepcopy\n",
    "    pair_counts_cp = deepcopy(pair_counts)\n",
    "    update_pair_counts(pair_counts_cp, itemset)\n",
    "    return pair_counts_cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_output_md"
    ]
   },
   "source": [
    "<!-- Expected demo output text block -->\n",
    "The demo included in the solution cell below should display the following output:\n",
    "```\n",
    "defaultdict(int,\n",
    "            {('o', 'e'): 1,\n",
    "             ('e', 'o'): 1,\n",
    "             ('o', 'r'): 2,\n",
    "             ('r', 'o'): 2,\n",
    "             ('e', 'r'): 1,\n",
    "             ('r', 'e'): 1,\n",
    "             ('f', 'o'): 1,\n",
    "             ('o', 'f'): 1,\n",
    "             ('f', 'r'): 1,\n",
    "             ('r', 'f'): 1,\n",
    "             ('f', 'g'): 1,\n",
    "             ('g', 'f'): 1,\n",
    "             ('o', 'g'): 1,\n",
    "             ('g', 'o'): 1,\n",
    "             ('r', 'g'): 1,\n",
    "             ('g', 'r'): 1})\n",
    "```\n",
    "> Note: This displayed output is `demo_pair_counts_ex4` which was updated _in place_. Your solution does not need to return any object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "exercise_solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('o', 'e'): 1,\n",
       "             ('e', 'o'): 1,\n",
       "             ('o', 'r'): 2,\n",
       "             ('r', 'o'): 2,\n",
       "             ('e', 'r'): 1,\n",
       "             ('r', 'e'): 1,\n",
       "             ('g', 'f'): 1,\n",
       "             ('f', 'g'): 1,\n",
       "             ('g', 'r'): 1,\n",
       "             ('r', 'g'): 1,\n",
       "             ('g', 'o'): 1,\n",
       "             ('o', 'g'): 1,\n",
       "             ('f', 'r'): 1,\n",
       "             ('r', 'f'): 1,\n",
       "             ('f', 'o'): 1,\n",
       "             ('o', 'f'): 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "\n",
    "def update_pair_counts (pair_counts, itemset):\n",
    "    \"\"\"\n",
    "    Updates a dictionary of pair counts for\n",
    "    all pairs of items in a given itemset.\n",
    "    \"\"\"\n",
    "    assert type (pair_counts) is defaultdict\n",
    "    item_pairs = combinations(itemset, 2)\n",
    "    \n",
    "    # Update pair_counts for each item-pair\n",
    "    for a,b in item_pairs:\n",
    "        pair_counts[(a,b)] += 1\n",
    "        pair_counts[(b,a)] += 1\n",
    "        \n",
    "        \n",
    "update_pair_counts_wrapper(demo_pair_counts_ex4, demo_itemset_ex4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(demo_pair_counts_ex4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "test_data_boilerplate"
    ]
   },
   "source": [
    "<!-- Test Cell Boilerplate -->\n",
    "The cell below will test your solution for Exercise 4. The testing variables will be available for debugging under the following names in a dictionary format.\n",
    "- `input_vars` - Input variables for your solution. \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
    "- `returned_output_vars` - Outputs returned by your solution.\n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex4",
     "locked": true,
     "points": "2",
     "solution": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### test_cell_ex4\n",
    "\n",
    "from tester_fw.testers import Tester\n",
    "\n",
    "conf = {\n",
    "    'case_file':'tc_4', \n",
    "    'func': update_pair_counts_wrapper, # replace this with the function defined above\n",
    "    'inputs':{ # input config dict. keys are parameter names\n",
    "        'pair_counts':{\n",
    "            'dtype':'defaultdict', # data type of param.\n",
    "            'check_modified':True,\n",
    "        },\n",
    "        'itemset':{\n",
    "            'dtype':'set', # data type of param.\n",
    "            'check_modified':False,\n",
    "        }\n",
    "    },\n",
    "    'outputs':{\n",
    "        'output_0':{\n",
    "            'index':0,\n",
    "            'dtype':'defaultdict',\n",
    "            'check_dtype': False,\n",
    "            'check_col_dtypes': False, # Ignored if dtype is not df\n",
    "            'check_col_order': False, # Ignored if dtype is not df\n",
    "            'check_row_order': False, # Ignored if dtype is not df\n",
    "            'check_column_type': False, # Ignored if dtype is not df\n",
    "            'float_tolerance': 10 ** (-6)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "tester = Tester(conf, key=b's63L2lglDfBJpcKzxpwcyy61HyKnJNBOJXl9BMyWhyo=', path='resource/asnlib/publicdata/')\n",
    "for _ in range(70):\n",
    "    try:\n",
    "        tester.run_test()\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "    except:\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "        raise\n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "exercise_text"
    ]
   },
   "source": [
    "**Exercise 5** (2 points). Implement a procedure that, given an itemset, updates a table to track counts of each item.\n",
    "\n",
    "As with the previous exercise, you may assume all items in the given itemset (`itemset`) are distinct, i.e., that you may treat it as you would any set-like collection. You may also assume the table (`item_counts`) is a default dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_data"
    ]
   },
   "outputs": [],
   "source": [
    "### Define demo inputs\n",
    "demo_item_counts_ex5 = defaultdict(int)\n",
    "demo_item_counts_ex5.update({'o': 1, 'e': 1, 'r': 1})\n",
    "demo_itemset_ex5 = {'f', 'r', 'o', 'g'}\n",
    "\n",
    "def update_item_counts_wrapper(item_counts, itemset):\n",
    "    from copy import deepcopy\n",
    "    item_counts_cp = deepcopy(item_counts)\n",
    "    update_item_counts(item_counts_cp, itemset)\n",
    "    return item_counts_cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_output_md"
    ]
   },
   "source": [
    "<!-- Expected demo output text block -->\n",
    "The demo included in the solution cell below should display the following output:\n",
    "```\n",
    "defaultdict(int, {'o': 2, 'e': 1, 'r': 2, 'f': 1, 'g': 1})\n",
    "```\n",
    "<!-- Include any shout outs here -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "exercise_solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'o': 2, 'e': 1, 'r': 2, 'g': 1, 'f': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_item_counts(item_counts, itemset):\n",
    "    for letter in itemset:\n",
    "        item_counts[letter] += 1\n",
    "        \n",
    "update_item_counts_wrapper(demo_item_counts_ex5, demo_itemset_ex5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "test_data_boilerplate"
    ]
   },
   "source": [
    "<!-- Test Cell Boilerplate -->\n",
    "The cell below will test your solution for Exercise 5. The testing variables will be available for debugging under the following names in a dictionary format.\n",
    "- `input_vars` - Input variables for your solution. \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
    "- `returned_output_vars` - Outputs returned by your solution.\n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex5",
     "locked": true,
     "points": "2",
     "solution": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### test_cell_ex5\n",
    "\n",
    "from tester_fw.testers import Tester\n",
    "\n",
    "conf = {\n",
    "    'case_file':'tc_5', \n",
    "    'func': update_item_counts_wrapper, # replace this with the function defined above\n",
    "    'inputs':{ # input config dict. keys are parameter names\n",
    "        'item_counts':{\n",
    "            'dtype':'defaultdict', # data type of param.\n",
    "            'check_modified':False,\n",
    "        },\n",
    "        'itemset':{\n",
    "            'dtype':'set', # data type of param.\n",
    "            'check_modified':False,\n",
    "        }\n",
    "    },\n",
    "    'outputs':{\n",
    "        'output_0':{\n",
    "            'index':0,\n",
    "            'dtype':'defaultdict',\n",
    "            'check_dtype': False,\n",
    "            'check_col_dtypes': False, # Ignored if dtype is not df\n",
    "            'check_col_order': False, # Ignored if dtype is not df\n",
    "            'check_row_order': False, # Ignored if dtype is not df\n",
    "            'check_column_type': False, # Ignored if dtype is not df\n",
    "            'float_tolerance': 10 ** (-6)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "tester = Tester(conf, key=b's63L2lglDfBJpcKzxpwcyy61HyKnJNBOJXl9BMyWhyo=', path='resource/asnlib/publicdata/')\n",
    "for _ in range(70):\n",
    "    try:\n",
    "        tester.run_test()\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "    except:\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "        raise\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "exercise_text"
    ]
   },
   "source": [
    "**Exercise 6** (2 points). Define `create_rules_from_counts` as follows: \n",
    "Given tables of item-pair counts (`pair_counts`) and individual item counts (`item_counts`) (You can assume both are default dictionaries), return all of the rules. The returned rules should be in the form of a dictionary whose key is the tuple, $(a, b)$ corresponding to the rule $a \\Rightarrow b$, and whose value is the confidence of the rule, $\\mathrm{conf}(a \\Rightarrow b)$. \n",
    "\n",
    "You may assume that if $(a, b)$ is in the table of item-pair counts, then both $a$ and $b$ are in the table of individual item counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_data"
    ]
   },
   "outputs": [],
   "source": [
    "## Define demo inputs\n",
    "\n",
    "demo_item_counts_ex7 = {'blue fish': 44, 'one fish': 47, 'red fish': 100, 'two fish': 74}\n",
    "demo_pair_counts_ex7 = {('blue fish', 'one fish'): 16,\n",
    "                        ('one fish', 'blue fish'): 16,\n",
    "                        ('blue fish', 'red fish'): 36,\n",
    "                        ('red fish', 'blue fish'): 36,\n",
    "                        ('blue fish', 'two fish'): 27,\n",
    "                        ('two fish', 'blue fish'): 27,\n",
    "                        ('one fish', 'red fish'): 38,\n",
    "                        ('red fish', 'one fish'): 38,\n",
    "                        ('one fish', 'two fish'): 28,\n",
    "                        ('two fish', 'one fish'): 28,\n",
    "                        ('red fish', 'two fish'): 59,\n",
    "                        ('two fish', 'red fish'): 59}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_output_md"
    ]
   },
   "source": [
    "<!-- Expected demo output text block -->\n",
    "The demo included in the solution cell below should display the following output:\n",
    "```\n",
    "{('blue fish', 'one fish'): 0.36363636363636365,\n",
    " ('one fish', 'blue fish'): 0.3404255319148936,\n",
    " ('blue fish', 'red fish'): 0.8181818181818182,\n",
    " ('red fish', 'blue fish'): 0.36,\n",
    " ('blue fish', 'two fish'): 0.6136363636363636,\n",
    " ('two fish', 'blue fish'): 0.36486486486486486,\n",
    " ('one fish', 'red fish'): 0.8085106382978723,\n",
    " ('red fish', 'one fish'): 0.38,\n",
    " ('one fish', 'two fish'): 0.5957446808510638,\n",
    " ('two fish', 'one fish'): 0.3783783783783784,\n",
    " ('red fish', 'two fish'): 0.59,\n",
    " ('two fish', 'red fish'): 0.7972972972972973}\n",
    "```\n",
    "<!-- Include any shout outs here -->\n",
    "> Note: The items in the \"counts\" dictionaries are phrases, not just letters! You don't need any special logic to handle this. It's just something to notice.  \n",
    "  \n",
    "> Note: If you did the division indirectly your result might not match \"exactly\". There's more in other notebooks on why that is. However, we have accounted for it in the testing, so don't worry as long as you're \"reasonably close\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "exercise_solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('blue fish', 'one fish'): 0.36363636363636365,\n",
       " ('one fish', 'blue fish'): 0.3404255319148936,\n",
       " ('blue fish', 'red fish'): 0.8181818181818182,\n",
       " ('red fish', 'blue fish'): 0.36,\n",
       " ('blue fish', 'two fish'): 0.6136363636363636,\n",
       " ('two fish', 'blue fish'): 0.36486486486486486,\n",
       " ('one fish', 'red fish'): 0.8085106382978723,\n",
       " ('red fish', 'one fish'): 0.38,\n",
       " ('one fish', 'two fish'): 0.5957446808510638,\n",
       " ('two fish', 'one fish'): 0.3783783783783784,\n",
       " ('red fish', 'two fish'): 0.59,\n",
       " ('two fish', 'red fish'): 0.7972972972972973}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_rules_from_counts(pair_counts, item_counts):\n",
    "    \"\"\"\n",
    "    Creates association rules from item-pair counts and individual item counts.\n",
    "    Returns a dictionary with rules as keys and their corresponding confidences as values.\n",
    "    \"\"\"\n",
    "    rules = {}\n",
    "    \n",
    "    for (a, b), pair_count in pair_counts.items():\n",
    "        confidence_a_to_b = pair_count / item_counts[a]\n",
    "        rules[(a, b)] = confidence_a_to_b\n",
    "        \n",
    "    return rules\n",
    "rules = create_rules_from_counts(demo_pair_counts_ex7, demo_item_counts_ex7)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "test_data_boilerplate"
    ]
   },
   "source": [
    "<!-- Test Cell Boilerplate -->\n",
    "The cell below will test your solution for Exercise 6. The testing variables will be available for debugging under the following names in a dictionary format.\n",
    "- `input_vars` - Input variables for your solution. \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
    "- `returned_output_vars` - Outputs returned by your solution.\n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex6",
     "locked": true,
     "points": "2",
     "solution": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### test_cell_ex6\n",
    "\n",
    "from tester_fw.testers import Tester\n",
    "\n",
    "conf = {\n",
    "    'case_file':'tc_6', \n",
    "    'func': create_rules_from_counts, # replace this with the function defined above\n",
    "    'inputs':{ # input config dict. keys are parameter names\n",
    "        'pair_counts':{\n",
    "            'dtype':'dict', # data type of param.\n",
    "            'check_modified':True,\n",
    "        },\n",
    "        'item_counts':{\n",
    "            'dtype':'dict', # data type of param.\n",
    "            'check_modified':True,\n",
    "        }\n",
    "    },\n",
    "    'outputs':{\n",
    "        'output_0':{\n",
    "            'index':0,\n",
    "            'dtype':'dict',\n",
    "            'check_dtype': True,\n",
    "            'check_col_dtypes': False, # Ignored if dtype is not df\n",
    "            'check_col_order': False, # Ignored if dtype is not df\n",
    "            'check_row_order': False, # Ignored if dtype is not df\n",
    "            'check_column_type': False, # Ignored if dtype is not df\n",
    "            'float_tolerance': 10 ** (-6)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "tester = Tester(conf, key=b's63L2lglDfBJpcKzxpwcyy61HyKnJNBOJXl9BMyWhyo=', path='resource/asnlib/publicdata/')\n",
    "for _ in range(70):\n",
    "    try:\n",
    "        tester.run_test()\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "    except:\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "        raise\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside: pretty printing the rules.** The output of rules above is a little messy; here's a little helper function that structures that output a little, which will be useful for both debugging and reporting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf(blue fish => red fish) = 0.818\n",
      "conf(one fish => red fish) = 0.809\n",
      "conf(two fish => red fish) = 0.797\n",
      "conf(blue fish => two fish) = 0.614\n",
      "conf(one fish => two fish) = 0.596\n",
      "conf(red fish => two fish) = 0.590\n",
      "conf(red fish => one fish) = 0.380\n",
      "conf(two fish => one fish) = 0.378\n",
      "conf(two fish => blue fish) = 0.365\n",
      "conf(blue fish => one fish) = 0.364\n",
      "conf(red fish => blue fish) = 0.360\n",
      "conf(one fish => blue fish) = 0.340\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gen_rule_str(a, b, val=None, val_fmt='{:.3f}', sep=\" = \"):\n",
    "    text = \"{} => {}\".format(a, b)\n",
    "    if val:\n",
    "        text = \"conf(\" + text + \")\"\n",
    "        text += sep + val_fmt.format(val)\n",
    "    return text\n",
    "\n",
    "def print_rules(rules):\n",
    "    if type(rules) is dict or type(rules) is defaultdict:\n",
    "        from operator import itemgetter\n",
    "        ordered_rules = sorted(rules.items(), key=itemgetter(1), reverse=True)\n",
    "    else: # Assume rules is iterable\n",
    "        ordered_rules = [((a, b), None) for a, b in rules]\n",
    "    for (a, b), conf_ab in ordered_rules:\n",
    "        print(gen_rule_str(a, b, conf_ab))\n",
    "\n",
    "# Demo:\n",
    "print_rules(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "exercise_text"
    ]
   },
   "source": [
    "**Exercise 7** (1 Point). Given `rules`, a dictionary mapping pairs `(a, b)` to the confidence that `a` implies `b` as well as a `threshold`, define the function `filter_rules_by_conf`. It should return all the rules whose confidence is _at least_ the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_data"
    ]
   },
   "outputs": [],
   "source": [
    "### Define demo inputs\n",
    "\n",
    "demo_rules_ex7 = {('blue fish', 'one fish'): 0.36363636363636365,\n",
    "                    ('one fish', 'blue fish'): 0.3404255319148936,\n",
    "                    ('blue fish', 'red fish'): 0.8181818181818182,\n",
    "                    ('red fish', 'blue fish'): 0.36,\n",
    "                    ('blue fish', 'two fish'): 0.6136363636363636,\n",
    "                    ('two fish', 'blue fish'): 0.36486486486486486,\n",
    "                    ('one fish', 'red fish'): 0.8085106382978723,\n",
    "                    ('red fish', 'one fish'): 0.38,\n",
    "                    ('one fish', 'two fish'): 0.5957446808510638,\n",
    "                    ('two fish', 'one fish'): 0.3783783783783784,\n",
    "                    ('red fish', 'two fish'): 0.59,\n",
    "                    ('two fish', 'red fish'): 0.7972972972972973}\n",
    "demo_threshold_ex7 = 0.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_output_md"
    ]
   },
   "source": [
    "<!-- Expected demo output text block -->\n",
    "The demo included in the solution cell below should display the following output:\n",
    "```\n",
    "{('blue fish', 'red fish'): 0.8181818181818182,\n",
    " ('blue fish', 'two fish'): 0.6136363636363636,\n",
    " ('one fish', 'red fish'): 0.8085106382978723,\n",
    " ('one fish', 'two fish'): 0.5957446808510638,\n",
    " ('red fish', 'two fish'): 0.59,\n",
    " ('two fish', 'red fish'): 0.7972972972972973}\n",
    "```\n",
    "<!-- Include any shout outs here -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "exercise_solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('blue fish', 'red fish'): 0.8181818181818182, ('blue fish', 'two fish'): 0.6136363636363636, ('one fish', 'red fish'): 0.8085106382978723, ('one fish', 'two fish'): 0.5957446808510638, ('red fish', 'two fish'): 0.59, ('two fish', 'red fish'): 0.7972972972972973}\n"
     ]
    }
   ],
   "source": [
    "def filter_rules_by_conf(rules, threshold):\n",
    "    filtered_rules = {}\n",
    "    for pair, confidence in rules.items():\n",
    "        if confidence >= threshold:\n",
    "            filtered_rules[pair] = confidence\n",
    "    return filtered_rules\n",
    "\n",
    "filtered_rules = filter_rules_by_conf(demo_rules_ex7, demo_threshold_ex7)\n",
    "print(filtered_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "test_data_boilerplate"
    ]
   },
   "source": [
    "<!-- Test Cell Boilerplate -->\n",
    "The cell below will test your solution for Exercise 7. The testing variables will be available for debugging under the following names in a dictionary format.\n",
    "- `input_vars` - Input variables for your solution. \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
    "- `returned_output_vars` - Outputs returned by your solution.\n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex7",
     "locked": true,
     "points": "1",
     "solution": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### test_cell_ex7\n",
    "from tester_fw.testers import Tester\n",
    "\n",
    "conf = {\n",
    "    'case_file':'tc_7', \n",
    "    'func': filter_rules_by_conf, # replace this with the function defined above\n",
    "    'inputs':{ # input config dict. keys are parameter names\n",
    "        'rules':{\n",
    "            'dtype':'dict', # data type of param.\n",
    "            'check_modified':True,\n",
    "        },\n",
    "        'threshold':{\n",
    "            'dtype':'float', # data type of param.\n",
    "            'check_modified':False,\n",
    "        }\n",
    "    },\n",
    "    'outputs':{\n",
    "        'output_0':{\n",
    "            'index':0,\n",
    "            'dtype':'dict',\n",
    "            'check_dtype': True,\n",
    "            'check_col_dtypes': False, # Ignored if dtype is not df\n",
    "            'check_col_order': False, # Ignored if dtype is not df\n",
    "            'check_row_order': False, # Ignored if dtype is not df\n",
    "            'check_column_type': False, # Ignored if dtype is not df\n",
    "            'float_tolerance': 0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "tester = Tester(conf, key=b's63L2lglDfBJpcKzxpwcyy61HyKnJNBOJXl9BMyWhyo=', path='resource/asnlib/publicdata/')\n",
    "for _ in range(70):\n",
    "    try:\n",
    "        tester.run_test()\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "    except:\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "        raise\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual baskets!\n",
    "\n",
    "Let's take a look at some real data that [someone](http://www.salemmarafi.com/code/market-basket-analysis-with-r/) was kind enough to prepare for a similar exercise designed for the R programming environment.\n",
    "\n",
    "First, here's a code snippet to load the data, which is a text file. If you are running in the Vocareum environment, we've already placed a copy of the data there; if you are running outside, this code will try to download a copy from the CSE 6040 website.\n",
    "\n",
    "Each line of this file is some customer's shopping basket. The items that the customer bought are stored as a comma-separated list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'groceries.csv' is ready!\n",
      "citrus fruit,semi-finished bread,margarine,ready soups\n",
      "tropical fruit,yogurt,coffee\n",
      "whole milk\n",
      "pip fruit,yogurt,cream cheese ,meat spreads\n",
      "other vegetables,whole milk,condensed milk,long life bakery product\n",
      "whole milk,butter,yogurt,rice,abrasive clea...\n",
      "... (etc.) ...\n",
      "\n",
      "(All data appears to be ready.)\n"
     ]
    }
   ],
   "source": [
    "def on_vocareum():\n",
    "    import os\n",
    "    return os.path.exists('.voc')\n",
    "\n",
    "def download(file, local_dir=\"\", url_base=None, checksum=None):\n",
    "    import os, requests, hashlib, io\n",
    "    local_file = \"{}{}\".format(local_dir, file)\n",
    "    if not os.path.exists(local_file):\n",
    "        if url_base is None:\n",
    "            url_base = \"https://cse6040.gatech.edu/datasets/\"\n",
    "        url = \"{}{}\".format(url_base, file)\n",
    "        print(\"Downloading: {} ...\".format(url))\n",
    "        r = requests.get(url)\n",
    "        with open(local_file, 'wb') as f:\n",
    "            f.write(r.content)            \n",
    "    if checksum is not None:\n",
    "        with io.open(local_file, 'rb') as f:\n",
    "            body = f.read()\n",
    "            body_checksum = hashlib.md5(body).hexdigest()\n",
    "            assert body_checksum == checksum, \\\n",
    "                \"Downloaded file '{}' has incorrect checksum: '{}' instead of '{}'\".format(local_file,\n",
    "                                                                                           body_checksum,\n",
    "                                                                                           checksum)\n",
    "    print(\"'{}' is ready!\".format(file))\n",
    "    \n",
    "if on_vocareum():\n",
    "    DATA_PATH = \"./resource/asnlib/publicdata/\"\n",
    "else:\n",
    "    DATA_PATH = \"\"\n",
    "datasets = {'groceries.csv': '0a3d21c692be5c8ce55c93e59543dcbe'}\n",
    "\n",
    "for filename, checksum in datasets.items():\n",
    "    download(filename, local_dir=DATA_PATH, checksum=checksum)\n",
    "\n",
    "with open('{}{}'.format(DATA_PATH, 'groceries.csv')) as fp:\n",
    "    groceries_file = fp.read()\n",
    "print (groceries_file[0:250] + \"...\\n... (etc.) ...\") # Prints the first 250 characters only\n",
    "print(\"\\n(All data appears to be ready.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "exercise_text"
    ]
   },
   "source": [
    "**Exercise 8** (3 Points). The groceries data is unfortunately in a different format than the letters we have been working with. We can process itemsets well enough with `update_pair_counts`, `update_item_counts`, `create_rules_from_counts`, and `filter_rules_by_conf`. Making the itemsets themselves is a different story. If we're going to work with this real data set then we have to make itemsets out of it. \n",
    "\n",
    "**Your task**: Complete the function `make_itemsets_csv`. Given `csv_str`, a string where each receipt is a _line_ and the items within each receipt are _separated by a comma_, return a list of sets. Each set should be a single receipt, and each element of the set should be an item contained within that receipt. As with the words the ordering within each itemset does not matter, however the order of the itemsets within the list should match the order in which they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_data"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "milk,eggs,peanut butter,oatmeal\n",
      "butter,pancake mix,maple syrup\n",
      "dog treats,milk,milk\n"
     ]
    }
   ],
   "source": [
    "### Define demo inputs\n",
    "\n",
    "demo_csv_str_ex8 = '''milk,eggs,peanut butter,oatmeal\n",
    "butter,pancake mix,maple syrup\n",
    "dog treats,milk,milk'''\n",
    "print(demo_csv_str_ex8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_output_md"
    ]
   },
   "source": [
    "<!-- Expected demo output text block -->\n",
    "The demo included in the solution cell below should display the following output:\n",
    "```\n",
    "[{'eggs', 'milk', 'oatmeal', 'peanut butter'},\n",
    " {'butter', 'maple syrup', 'pancake mix'},\n",
    " {'dog treats', 'milk'}]\n",
    "```\n",
    "<!-- Include any shout outs here -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "exercise_solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'eggs', 'milk', 'oatmeal', 'peanut butter'},\n",
       " {'butter', 'maple syrup', 'pancake mix'},\n",
       " {'dog treats', 'milk'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ex8 solution\n",
    "def make_itemsets_csv(csv_str):\n",
    "   \n",
    "    lines = csv_str.split('\\n')\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        line = line.split(',')\n",
    "        data.append(set(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "make_itemsets_csv(demo_csv_str_ex8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "test_data_boilerplate"
    ]
   },
   "source": [
    "<!-- Test Cell Boilerplate -->\n",
    "The cell below will test your solution for Exercise 8. The testing variables will be available for debugging under the following names in a dictionary format.\n",
    "- `input_vars` - Input variables for your solution. \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
    "- `returned_output_vars` - Outputs returned by your solution.\n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex8",
     "locked": true,
     "points": "3",
     "solution": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### test_cell_ex8\n",
    "from tester_fw.testers import Tester\n",
    "\n",
    "conf = {\n",
    "    'case_file':'tc_8', \n",
    "    'func': make_itemsets_csv, # replace this with the function defined above\n",
    "    'inputs':{ # input config dict. keys are parameter names\n",
    "        'csv_str':{\n",
    "            'dtype':'str', # data type of param.\n",
    "            'check_modified':False,\n",
    "        }\n",
    "    },\n",
    "    'outputs':{\n",
    "        'output_0':{\n",
    "            'index':0,\n",
    "            'dtype':'',\n",
    "            'check_dtype': True,\n",
    "            'check_col_dtypes': False, # Ignored if dtype is not df\n",
    "            'check_col_order': False, # Ignored if dtype is not df\n",
    "            'check_row_order': False, # Ignored if dtype is not df\n",
    "            'check_column_type': False, # Ignored if dtype is not df\n",
    "            'float_tolerance': 10 ** (-6)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "tester = Tester(conf, key=b's63L2lglDfBJpcKzxpwcyy61HyKnJNBOJXl9BMyWhyo=', path='resource/asnlib/publicdata/')\n",
    "for _ in range(70):\n",
    "    try:\n",
    "        tester.run_test()\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "    except:\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "        raise\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On higher order functions**  \n",
    "Python functions are *objects*, just like strings, lists, dictionaries, etc. This means that they can be passed as arguments to other functions. (You have seen this before with sorting in Notebook 1.) Here's a short example, say I have this function.\n",
    "\n",
    "```\n",
    "def do_math_operation(x, y, func):\n",
    "    return func(x, y)\n",
    "```\n",
    "\n",
    "Then I can define other functions and pass them to `do_math_operation` to change how it works. For instance let's say I have `add(a, b)` which returns the sum of `a` and `b`. I can call `do_math_operation(4, 5, add)` and the result will be the sum of `4+5`. I could also define a funciton called a function which multiplies two numbers pass that to change what `do_math_operation` does. For sure, that's not a very interesting example, but this is a very powerful concept. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "exercise_text"
    ]
   },
   "source": [
    "**Exercise 9** (4 Points). Use the tools you have created above to create a association rules from a data source. The following inputs:  \n",
    "- `source`: the source data for your rules. You can assume this will be _either_ structured like the `latin_text` _or_ like the csv-formatted `groceries_file`. \n",
    "- `itemset_maker`: a function which can be used to transform the `source` into \"itemsets\". *Since you are given the parsing function as a parameter, you do not need to attempt to determine the format of `source`*. See \"On higher order functions\" above.\n",
    "- `conf_threshold` and `min_count`: Your result should only include rules where `a` occurs in _at least_ `min_count` receipts _and_ $\\mathrm{conf}(a \\Rightarrow b)$ is at least `conf_threshold`\n",
    "\n",
    "Return your confidence rules as a dictionary.\n",
    "- Keys: pairs `(a,b)`\n",
    "- Values: $\\mathrm{conf}(a \\Rightarrow b)$\n",
    "\n",
    "> Note: We will test your solution using `make_itemsets_unstructured_text` and `make_itemsets_csv` on random inputs. These functions must be correctly defined above to pass the test cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_output_md"
    ]
   },
   "source": [
    "<!-- Expected demo output text block -->\n",
    "The demo included in the solution cell below should display the following output:\n",
    "```\n",
    "Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0\n",
    "conf(q => u) = 1.000\n",
    "conf(x => e) = 1.000\n",
    "conf(h => i) = 0.833\n",
    "conf(x => i) = 0.833\n",
    "conf(v => t) = 0.818\n",
    "conf(r => e) = 0.800\n",
    "conf(v => e) = 0.773\n",
    "conf(b => i) = 0.750\n",
    "conf(g => i) = 0.750\n",
    "conf(f => i) = 0.750\n",
    "\n",
    "Source: `groceries_file`; Itemset Maker: `make_itemsets_csv`; Confidence Threshold: 0.5; Min Count: 10\n",
    "conf(honey => whole milk) = 0.733\n",
    "conf(frozen fruits => other vegetables) = 0.667\n",
    "conf(cereals => whole milk) = 0.643\n",
    "conf(rice => whole milk) = 0.613\n",
    "conf(rubbing alcohol => whole milk) = 0.600\n",
    "conf(cocoa drinks => whole milk) = 0.591\n",
    "conf(pudding powder => whole milk) = 0.565\n",
    "conf(jam => whole milk) = 0.547\n",
    "conf(cream => other vegetables) = 0.538\n",
    "conf(cream => sausage) = 0.538\n",
    "conf(baking powder => whole milk) = 0.523\n",
    "conf(tidbits => rolls/buns) = 0.522\n",
    "conf(rice => other vegetables) = 0.520\n",
    "conf(cooking chocolate => whole milk) = 0.520\n",
    "conf(specialty cheese => other vegetables) = 0.500\n",
    "conf(rubbing alcohol => butter) = 0.500\n",
    "conf(rubbing alcohol => citrus fruit) = 0.500\n",
    "conf(ready soups => rolls/buns) = 0.500\n",
    "conf(frozen fruits => whipped/sour cream) = 0.500\n",
    "```\n",
    "<!-- Include any shout outs here -->\n",
    "\n",
    "> Note: The \"Source: ...; Itemset Maker: ...; Confidence Threshold: ...; Min Count: ...\" is just printed for convenience. It _should not be part of your output_!  \n",
    "  \n",
    "> Note: The rules are \"pretty printed\" using the `print_rules` function defined above. Your solution should output lists of dictionaries.  \n",
    "  \n",
    "> Note: The demo includes _two calls_ to your solution. The first is for `latin_rules`, and the second is for `groceries_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pair_counts = defaultdict(int)\n",
    "item_counts = defaultdict(int)\n",
    "letters = make_itemsets_unstructured_text(latin_text)\n",
    "\n",
    "\n",
    "for letter in letters:\n",
    "    update_pair_counts(pair_counts, letter)\n",
    "    update_item_counts(item_counts, letter)\n",
    "    \n",
    "new_item_counts = {}\n",
    "for key, value in item_counts.items():\n",
    "        new_item_counts[key] = value\n",
    "        \n",
    "item_counts = new_item_counts\n",
    "\n",
    "rules = {}\n",
    "for key, value in pair_counts.items():\n",
    "    if key[0] in item_counts:\n",
    "        rules[key] = value/item_counts[key[0]]\n",
    "        \n",
    "filtered_rules = {}\n",
    "filtered_rules=filter_rules_by_conf(rules, 0.75)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('r', 'e'): 0.8,\n",
       " ('v', 't'): 0.8181818181818182,\n",
       " ('v', 'e'): 0.7727272727272727,\n",
       " ('q', 'u'): 1.0,\n",
       " ('h', 'i'): 0.8333333333333334,\n",
       " ('x', 'e'): 1.0,\n",
       " ('x', 'i'): 0.8333333333333334,\n",
       " ('b', 'i'): 0.75,\n",
       " ('f', 'i'): 0.75,\n",
       " ('g', 'i'): 0.75}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'d', 'e', 's'},\n",
       " {'t', 'u'},\n",
       " {'a', 'c', 'e', 'i', 'p', 'r', 's', 't'},\n",
       " {'d', 'e', 'n', 'u'},\n",
       " {'i', 'm', 'n', 'o', 's'},\n",
       " {'e', 'i', 's', 't'},\n",
       " {'a', 'n', 's', 't', 'u'},\n",
       " {'e', 'o', 'r'},\n",
       " {'i', 's', 't'},\n",
       " {'a', 'e', 'l', 'm', 'o', 'p', 't', 'u', 'v'},\n",
       " {'a', 'c', 'i', 'm', 'n', 's', 't', 'u'},\n",
       " {'d', 'e', 'l', 'm', 'o', 'q', 'r', 'u'},\n",
       " {'a', 'd', 'i', 'l', 'm', 'n', 't', 'u'},\n",
       " {'a', 'm', 'o', 't'},\n",
       " {'e', 'm', 'r'},\n",
       " {'a', 'e', 'i', 'm', 'p', 'r'},\n",
       " {'a', 'e', 'q', 'u'},\n",
       " {'a', 'i', 'p', 's'},\n",
       " {'a', 'e', 'q', 'u'},\n",
       " {'a', 'b'},\n",
       " {'i', 'l', 'o'},\n",
       " {'e', 'i', 'n', 'o', 'r', 't', 'v'},\n",
       " {'a', 'e', 'i', 'r', 's', 't', 'v'},\n",
       " {'e', 't'},\n",
       " {'a', 'i', 'q', 's', 'u'},\n",
       " {'a', 'c', 'e', 'h', 'i', 'o', 'r', 't'},\n",
       " {'a', 'b', 'e', 't'},\n",
       " {'a', 'e', 'i', 't', 'v'},\n",
       " {'a', 'c', 'd', 'i', 't'},\n",
       " {'n', 's', 't', 'u'},\n",
       " {'a', 'b', 'c', 'e', 'i', 'l', 'o', 'p', 'x'},\n",
       " {'e', 'm', 'n', 'o'},\n",
       " {'e', 'i', 'm', 'n'},\n",
       " {'a', 'i', 'm', 'p', 's'},\n",
       " {'a', 'e', 'l', 'm', 'o', 'p', 't', 'u', 'v'},\n",
       " {'a', 'i', 'q', 'u'},\n",
       " {'a', 'l', 'o', 'p', 's', 't', 'u', 'v'},\n",
       " {'i', 's', 't'},\n",
       " {'a', 'e', 'n', 'p', 'r', 's', 't', 'u'},\n",
       " {'a', 't', 'u'},\n",
       " {'d', 'i', 'o', 't'},\n",
       " {'a', 't', 'u'},\n",
       " {'f', 'g', 'i', 't', 'u'},\n",
       " {'d', 'e', 's'},\n",
       " {'a', 'i', 'q', 'u'},\n",
       " {'c', 'e', 'n', 'o', 'q', 'r', 's', 't', 'u'},\n",
       " {'a', 'g', 'i', 'm', 'n'},\n",
       " {'d', 'e', 'l', 'o', 'r', 's'},\n",
       " {'e', 'o', 's'},\n",
       " {'i', 'q', 'u'},\n",
       " {'a', 'e', 'i', 'n', 'o', 'r', 't'},\n",
       " {'a', 'e', 'l', 'm', 'o', 'p', 't', 'u', 'v'},\n",
       " {'e', 'i', 'q', 's', 'u'},\n",
       " {'c', 'e', 'i', 'n', 's', 't', 'u'},\n",
       " {'e', 'n', 'q', 'u'},\n",
       " {'o', 'p', 'r'},\n",
       " {'a', 'i', 'm', 'q', 's', 'u'},\n",
       " {'e', 's', 't'},\n",
       " {'i', 'q', 'u'},\n",
       " {'d', 'e', 'l', 'm', 'o', 'r'},\n",
       " {'i', 'm', 'p', 's', 'u'},\n",
       " {'a', 'i', 'q', 'u'},\n",
       " {'d', 'l', 'o', 'r'},\n",
       " {'i', 's', 't'},\n",
       " {'a', 'e', 'm', 't'},\n",
       " {'c', 'e', 'n', 'o', 'r', 's', 't', 'u'},\n",
       " {'a', 'c', 'd', 'g', 'i', 'n', 'p', 's'},\n",
       " {'e', 'i', 'l', 't', 'v'},\n",
       " {'d', 'e', 's'},\n",
       " {'a', 'i', 'q', 'u'},\n",
       " {'n', 'o'},\n",
       " {'a', 'm', 'n', 'q', 'u'},\n",
       " {'d', 'o'},\n",
       " {'e', 'i', 's', 'u'},\n",
       " {'d', 'i', 'm', 'o'},\n",
       " {'a', 'e', 'm', 'o', 'p', 'r', 't'},\n",
       " {'c', 'd', 'i', 'n', 't', 'u'},\n",
       " {'t', 'u'},\n",
       " {'a', 'b', 'e', 'l', 'o', 'r'},\n",
       " {'e', 't'},\n",
       " {'d', 'e', 'l', 'o', 'r'},\n",
       " {'a', 'g', 'm', 'n'},\n",
       " {'a', 'i', 'l', 'm', 'q', 'u'},\n",
       " {'a', 'e', 'q', 'r', 't', 'u'},\n",
       " {'a', 'e', 'l', 'm', 'o', 'p', 't', 'u', 'v'},\n",
       " {'t', 'u'},\n",
       " {'e', 'i', 'm', 'n'},\n",
       " {'a', 'd'},\n",
       " {'a', 'i', 'm', 'n'},\n",
       " {'a', 'e', 'i', 'm', 'n', 'v'},\n",
       " {'i', 'q', 's', 'u'},\n",
       " {'m', 'n', 'o', 'r', 's', 't', 'u'},\n",
       " {'a', 'c', 'e', 'i', 'm', 'n', 'o', 'r', 't', 'x'},\n",
       " {'a', 'l', 'm', 'u'},\n",
       " {'c', 'i', 'o', 'p', 'r', 's'},\n",
       " {'c', 'i', 'p', 's', 't', 'u'},\n",
       " {'a', 'b', 'i', 'l', 'm', 'o', 'r', 's'},\n",
       " {'i', 'n', 's'},\n",
       " {'t', 'u'},\n",
       " {'a', 'd', 'i', 'l', 'q', 'u'},\n",
       " {'e', 'x'},\n",
       " {'a', 'e'},\n",
       " {'c', 'd', 'i', 'm', 'o'},\n",
       " {'a', 'c', 'e', 'n', 'o', 'q', 'r', 's', 't', 'u'},\n",
       " {'i', 'q', 's', 'u'},\n",
       " {'a', 'e', 'm', 't', 'u'},\n",
       " {'e', 'l', 'v'},\n",
       " {'e', 'm', 'u'},\n",
       " {'e', 'i', 'r', 'u'},\n",
       " {'d', 'e', 'h', 'i', 'n', 'p', 'r', 't'},\n",
       " {'i', 'q', 'u'},\n",
       " {'i', 'n'},\n",
       " {'a', 'e'},\n",
       " {'a', 'e', 'l', 'o', 'p', 't', 'u', 'v'},\n",
       " {'e', 'i', 'l', 't', 'v'},\n",
       " {'e', 's'},\n",
       " {'a', 'm', 'q', 'u'},\n",
       " {'h', 'i', 'l', 'n'},\n",
       " {'a', 'e', 'i', 'l', 'm', 'o', 's', 't'},\n",
       " {'a', 'c', 'e', 'n', 'o', 'q', 'r', 's', 't', 'u'},\n",
       " {'e', 'l', 'v'},\n",
       " {'i', 'l', 'm', 'u'},\n",
       " {'i', 'q', 'u'},\n",
       " {'d', 'e', 'l', 'm', 'o', 'r'},\n",
       " {'e', 'm', 'u'},\n",
       " {'a', 'f', 'g', 'i', 't', 'u'},\n",
       " {'o', 'q', 'u'},\n",
       " {'a', 'l', 'o', 'p', 's', 't', 'u', 'v'},\n",
       " {'a', 'l', 'n', 'u'},\n",
       " {'a', 'i', 'p', 'r', 't', 'u'},\n",
       " {'a', 't'},\n",
       " {'e', 'o', 'r', 'v'},\n",
       " {'e', 'o', 's'},\n",
       " {'e', 't'},\n",
       " {'a', 'c', 'm', 's', 'u'},\n",
       " {'e', 't'},\n",
       " {'i', 'o', 's', 't', 'u'},\n",
       " {'d', 'i', 'o'},\n",
       " {'d', 'g', 'i', 'm', 'n', 'o', 's'},\n",
       " {'c', 'd', 'i', 'm', 's', 'u'},\n",
       " {'i', 'q', 'u'},\n",
       " {'a', 'b', 'd', 'i', 'l', 'n', 's', 't'},\n",
       " {'a', 'e', 'i', 'm', 'n', 'p', 'r', 's', 't', 'u'},\n",
       " {'a', 'l', 'm', 'o', 'p', 't', 'u', 'v'},\n",
       " {'d', 'e', 'i', 'l', 'n', 't'},\n",
       " {'a', 'e', 'q', 't', 'u'},\n",
       " {'c', 'i', 'o', 'p', 'r', 't', 'u'},\n",
       " {'o', 'q', 's', 'u'},\n",
       " {'d', 'e', 'l', 'o', 'r', 's'},\n",
       " {'e', 't'},\n",
       " {'a', 'q', 's', 'u'},\n",
       " {'a', 'e', 'i', 'l', 'm', 'o', 's', 't'},\n",
       " {'c', 'e', 'i', 'p', 'r', 't', 'u', 'x'},\n",
       " {'i', 'n', 's', 't'},\n",
       " {'a', 'b', 'c', 'e', 'i', 'o', 't'},\n",
       " {'a', 'c', 'd', 'e', 'i', 'p', 't', 'u'},\n",
       " {'n', 'o'},\n",
       " {'d', 'e', 'i', 'n', 'o', 'p', 'r', 't', 'v'},\n",
       " {'e', 'i', 'l', 'm', 'q', 's', 'u'},\n",
       " {'n', 's', 't', 'u'},\n",
       " {'i', 'n'},\n",
       " {'a', 'c', 'l', 'p', 'u'},\n",
       " {'i', 'q', 'u'},\n",
       " {'a', 'c', 'f', 'i', 'o'},\n",
       " {'d', 'e', 'n', 'r', 's', 't', 'u'},\n",
       " {'a', 'i', 'l', 'm', 'o', 't'},\n",
       " {'a', 'i', 'm', 'n'},\n",
       " {'d', 'i'},\n",
       " {'e', 's', 't'},\n",
       " {'a', 'b', 'l', 'm', 'o', 'r', 'u'},\n",
       " {'e', 't'},\n",
       " {'d', 'l', 'm', 'o', 'r', 'u'},\n",
       " {'a', 'f', 'g', 'u'},\n",
       " {'e', 't'},\n",
       " {'a', 'h', 'm', 'r', 'u'},\n",
       " {'d', 'e', 'i', 'm', 'q', 'u'},\n",
       " {'e', 'm', 'r', 'u'},\n",
       " {'a', 'c', 'f', 'i', 'l', 's'},\n",
       " {'e', 's', 't'},\n",
       " {'e', 't'},\n",
       " {'a', 'd', 'e', 'i', 'p', 't', 'x'},\n",
       " {'c', 'd', 'i', 'n', 'o', 's', 't'},\n",
       " {'a', 'm', 'n'},\n",
       " {'b', 'e', 'i', 'l', 'o', 'r'},\n",
       " {'e', 'm', 'o', 'p', 'r', 't'},\n",
       " {'c', 'm', 'u'},\n",
       " {'a', 'l', 'o', 's', 't', 'u'},\n",
       " {'b', 'i', 'n', 'o', 's'},\n",
       " {'e', 's', 't'},\n",
       " {'d', 'e', 'g', 'i', 'l', 'n'},\n",
       " {'i', 'o', 'p', 't'},\n",
       " {'c', 'e', 'm', 'q', 'u'},\n",
       " {'h', 'i', 'l', 'n'},\n",
       " {'d', 'e', 'i', 'm', 'p', 't'},\n",
       " {'o', 'q', 'u'},\n",
       " {'i', 'm', 'n', 's', 'u'},\n",
       " {'d', 'i'},\n",
       " {'d', 'o', 'q', 'u'},\n",
       " {'a', 'e', 'i', 'm', 'x'},\n",
       " {'a', 'c', 'e', 'l', 'p', 't'},\n",
       " {'a', 'c', 'e', 'f', 'r'},\n",
       " {'i', 'm', 'o', 'p', 's', 'u'},\n",
       " {'i', 'm', 'n', 'o', 's'},\n",
       " {'a', 'l', 'o', 'p', 's', 't', 'u', 'v'},\n",
       " {'a', 'd', 'e', 'm', 'n', 's', 'u'},\n",
       " {'e', 's', 't'},\n",
       " {'i', 'm', 'n', 'o', 's'},\n",
       " {'d', 'l', 'o', 'r'},\n",
       " {'d', 'e', 'l', 'n', 'p', 'r', 's', 'u'},\n",
       " {'b', 'e', 'i', 'm', 'o', 'p', 'r', 's', 't', 'u'},\n",
       " {'a', 'e', 'm', 't', 'u'},\n",
       " {'a', 'b', 'd', 'i', 'm', 'q', 's', 'u'},\n",
       " {'e', 't'},\n",
       " {'a', 't', 'u'},\n",
       " {'c', 'f', 'i', 'o', 's'},\n",
       " {'b', 'd', 'e', 'i', 's', 't'},\n",
       " {'a', 't', 'u'},\n",
       " {'e', 'm', 'r', 'u'},\n",
       " {'a', 'b', 'c', 'e', 'i', 'n', 's', 't', 'u'},\n",
       " {'a', 'e', 'p', 's'},\n",
       " {'e', 'i', 'n', 't', 'v'},\n",
       " {'t', 'u'},\n",
       " {'e', 't'},\n",
       " {'a', 'e', 'l', 'o', 'p', 's', 't', 'u', 'v'},\n",
       " {'a', 'd', 'e', 'i', 'n', 'p', 'r', 'u'},\n",
       " {'i', 'n', 's', 't'},\n",
       " {'e', 't'},\n",
       " {'a', 'e', 'i', 'l', 'm', 'o', 's', 't'},\n",
       " {'n', 'o'},\n",
       " {'a', 'c', 'd', 'e', 'n', 'r', 's', 'u'},\n",
       " {'a', 'e', 'i', 'q', 't', 'u'},\n",
       " {'a', 'e', 'm', 'r', 'u'},\n",
       " {'e', 'm', 'r', 'u'},\n",
       " {'c', 'h', 'i'},\n",
       " {'e', 'n', 'r', 't', 'u'},\n",
       " {'a'},\n",
       " {'a', 'e', 'i', 'n', 'p', 's', 't'},\n",
       " {'c', 'd', 'e', 'l', 's', 't', 'u'},\n",
       " {'t', 'u'},\n",
       " {'a', 't', 'u'},\n",
       " {'c', 'd', 'e', 'i', 'n', 'r', 's'},\n",
       " {'a', 'b', 'i', 'l', 'o', 'p', 's', 't', 'u', 'v'},\n",
       " {'a', 'e', 'i', 'm', 'o', 'r', 's'},\n",
       " {'a', 'i', 'l', 's'},\n",
       " {'a', 'c', 'e', 'n', 'o', 'q', 'r', 's', 't', 'u'},\n",
       " {'a', 't', 'u'},\n",
       " {'d', 'e', 'f', 'i', 'n', 'p', 'r', 's'},\n",
       " {'b', 'd', 'i', 'l', 'o', 'r', 's', 'u'},\n",
       " {'a', 'e', 'i', 'o', 'p', 'r', 's'},\n",
       " {'a', 'e', 'l', 'p', 'r', 't'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0\n",
      "conf(q => u) = 1.000\n",
      "conf(x => e) = 1.000\n",
      "conf(h => i) = 0.833\n",
      "conf(x => i) = 0.833\n",
      "conf(v => t) = 0.818\n",
      "conf(r => e) = 0.800\n",
      "conf(v => e) = 0.773\n",
      "conf(b => i) = 0.750\n",
      "conf(f => i) = 0.750\n",
      "conf(g => i) = 0.750\n",
      "\n",
      "Source: `groceries_file`; Itemset Maker: `make_itemsets_csv`; Confidence Threshold: 0.5; Min Count: 10\n",
      "conf(honey => whole milk) = 0.733\n",
      "conf(frozen fruits => other vegetables) = 0.667\n",
      "conf(cereals => whole milk) = 0.643\n",
      "conf(rice => whole milk) = 0.613\n",
      "conf(rubbing alcohol => whole milk) = 0.600\n",
      "conf(cocoa drinks => whole milk) = 0.591\n",
      "conf(pudding powder => whole milk) = 0.565\n",
      "conf(jam => whole milk) = 0.547\n",
      "conf(cream => other vegetables) = 0.538\n",
      "conf(cream => sausage) = 0.538\n",
      "conf(baking powder => whole milk) = 0.523\n",
      "conf(tidbits => rolls/buns) = 0.522\n",
      "conf(rice => other vegetables) = 0.520\n",
      "conf(cooking chocolate => whole milk) = 0.520\n",
      "conf(specialty cheese => other vegetables) = 0.500\n",
      "conf(rubbing alcohol => butter) = 0.500\n",
      "conf(rubbing alcohol => citrus fruit) = 0.500\n",
      "conf(ready soups => rolls/buns) = 0.500\n",
      "conf(frozen fruits => whipped/sour cream) = 0.500\n"
     ]
    }
   ],
   "source": [
    "def create_rules_from_source(source, itemset_maker, conf_threshold=0, min_count=0):\n",
    "    pair_counts = defaultdict(int)\n",
    "    item_counts = defaultdict(int)\n",
    "\n",
    "    \n",
    "    receipts = itemset_maker(source)\n",
    "\n",
    "\n",
    "  \n",
    "    for receipt in receipts:\n",
    "        update_pair_counts(pair_counts, receipt)\n",
    "        update_item_counts(item_counts, receipt)\n",
    "\n",
    "        \n",
    "\n",
    "    new_item_counts = {}\n",
    "    for key, value in item_counts.items():\n",
    "        if value >= min_count:\n",
    "            new_item_counts[key] = value\n",
    "       \n",
    "    item_counts = new_item_counts\n",
    "\n",
    "\n",
    "    rules = {}\n",
    "    for key, value in pair_counts.items():\n",
    "        if key[0] in item_counts:\n",
    "            rules[key] = value/item_counts[key[0]]\n",
    "\n",
    "\n",
    "    filtered_rules = {}\n",
    "    filtered_rules = filter_rules_by_conf(rules, conf_threshold)\n",
    "    \n",
    "\n",
    "    return filtered_rules\n",
    "    \n",
    "    \n",
    "latin_rules = create_rules_from_source(latin_text, make_itemsets_unstructured_text, 0.75)\n",
    "grocery_rules = create_rules_from_source(groceries_file, make_itemsets_csv, 0.5, 10)\n",
    "print('Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0')\n",
    "print_rules(latin_rules)\n",
    "print()\n",
    "print('Source: `groceries_file`; Itemset Maker: `make_itemsets_csv`; Confidence Threshold: 0.5; Min Count: 10')\n",
    "print_rules(grocery_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "exercise_solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0\n",
      "conf(q => u) = 1.000\n",
      "conf(x => e) = 1.000\n",
      "conf(h => i) = 0.833\n",
      "conf(x => i) = 0.833\n",
      "conf(v => t) = 0.818\n",
      "conf(r => e) = 0.800\n",
      "conf(v => e) = 0.773\n",
      "conf(b => i) = 0.750\n",
      "conf(f => i) = 0.750\n",
      "conf(g => i) = 0.750\n",
      "\n",
      "Source: `groceries_file`; Itemset Maker: `make_itemsets_csv`; Confidence Threshold: 0.5; Min Count: 10\n",
      "conf(honey => whole milk) = 0.733\n",
      "conf(frozen fruits => other vegetables) = 0.667\n",
      "conf(cereals => whole milk) = 0.643\n",
      "conf(rice => whole milk) = 0.613\n",
      "conf(rubbing alcohol => whole milk) = 0.600\n",
      "conf(cocoa drinks => whole milk) = 0.591\n",
      "conf(pudding powder => whole milk) = 0.565\n",
      "conf(jam => whole milk) = 0.547\n",
      "conf(cream => other vegetables) = 0.538\n",
      "conf(cream => sausage) = 0.538\n",
      "conf(baking powder => whole milk) = 0.523\n",
      "conf(tidbits => rolls/buns) = 0.522\n",
      "conf(rice => other vegetables) = 0.520\n",
      "conf(cooking chocolate => whole milk) = 0.520\n",
      "conf(specialty cheese => other vegetables) = 0.500\n",
      "conf(rubbing alcohol => butter) = 0.500\n",
      "conf(rubbing alcohol => citrus fruit) = 0.500\n",
      "conf(ready soups => rolls/buns) = 0.500\n",
      "conf(frozen fruits => whipped/sour cream) = 0.500\n"
     ]
    }
   ],
   "source": [
    "def create_rules_from_source(source, itemset_maker, conf_threshold=0, min_count=0):\n",
    "    pair_counts = defaultdict(int)\n",
    "    item_counts = defaultdict(int)\n",
    "\n",
    "    \n",
    "    receipts = itemset_maker(source)\n",
    "\n",
    "\n",
    "  \n",
    "    for receipt in receipts:\n",
    "        update_pair_counts(pair_counts, receipt)\n",
    "        update_item_counts(item_counts, receipt)\n",
    "\n",
    "        \n",
    "\n",
    "    new_item_counts = {}\n",
    "    for key, value in item_counts.items():\n",
    "        if value >= min_count:\n",
    "            new_item_counts[key] = value\n",
    "       \n",
    "    item_counts = new_item_counts\n",
    "\n",
    "\n",
    "    rules = {}\n",
    "    for key, value in pair_counts.items():\n",
    "        if key[0] in item_counts:\n",
    "            rules[key] = value/item_counts[key[0]]\n",
    "\n",
    "\n",
    "    filtered_rules = {}\n",
    "    filtered_rules = filter_rules_by_conf(rules, conf_threshold)\n",
    "    \n",
    "\n",
    "    return filtered_rules\n",
    "    \n",
    "    \n",
    "latin_rules = create_rules_from_source(latin_text, make_itemsets_unstructured_text, 0.75)\n",
    "grocery_rules = create_rules_from_source(groceries_file, make_itemsets_csv, 0.5, 10)\n",
    "print('Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0')\n",
    "print_rules(latin_rules)\n",
    "print()\n",
    "print('Source: `groceries_file`; Itemset Maker: `make_itemsets_csv`; Confidence Threshold: 0.5; Min Count: 10')\n",
    "print_rules(grocery_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "test_data_boilerplate"
    ]
   },
   "source": [
    "<!-- Test Cell Boilerplate -->\n",
    "The cell below will test your solution for Exercise 9. The testing variables will be available for debugging under the following names in a dictionary format.\n",
    "- `input_vars` - Input variables for your solution. \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
    "- `returned_output_vars` - Outputs returned by your solution.\n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex9",
     "locked": true,
     "points": "4",
     "solution": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### test_cell_ex9\n",
    "from tester_fw.testers import Tester\n",
    "\n",
    "conf = {\n",
    "    'case_file':'tc_9', \n",
    "    'func': create_rules_from_source, # replace this with the function defined above\n",
    "    'inputs':{ # input config dict. keys are parameter names\n",
    "        'source':{\n",
    "            'dtype':'string', # data type of param.\n",
    "            'check_modified':False,\n",
    "        },\n",
    "        'itemset_maker':{\n",
    "            'dtype':'function', # data type of param.\n",
    "            'check_modified':False,\n",
    "        },\n",
    "        'conf_threshold':{\n",
    "            'dtype':'float', # data type of param.\n",
    "            'check_modified':False,\n",
    "        },\n",
    "        'min_count':{\n",
    "            'dtype':'int', # data type of param.\n",
    "            'check_modified':False,\n",
    "        }\n",
    "    },\n",
    "    'outputs':{\n",
    "        'output_0':{\n",
    "            'index':0,\n",
    "            'dtype':'dict',\n",
    "            'check_dtype': True,\n",
    "            'check_col_dtypes': False, # Ignored if dtype is not df\n",
    "            'check_col_order': False, # Ignored if dtype is not df\n",
    "            'check_row_order': False, # Ignored if dtype is not df\n",
    "            'check_column_type': False, # Ignored if dtype is not df\n",
    "            'float_tolerance': 10 ** (-6)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "tester = Tester(conf, key=b's63L2lglDfBJpcKzxpwcyy61HyKnJNBOJXl9BMyWhyo=', path='resource/asnlib/publicdata/')\n",
    "for _ in range(70):\n",
    "    try:\n",
    "        tester.run_test()\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "    except:\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "        raise\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside** In case you \\*cough\\* didn't get around to \\*cough\\* looking up and reading the translation of the Lorem Ipsum, here's the first two paragraphs. We're going to use it in the next exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_text = \"\"\"\n",
    "But I must explain to you how all this mistaken idea\n",
    "of denouncing of a pleasure and praising pain was\n",
    "born and I will give you a complete account of the\n",
    "system, and expound the actual teachings of the great\n",
    "explorer of the truth, the master-builder of human\n",
    "happiness. No one rejects, dislikes, or avoids\n",
    "pleasure itself, because it is pleasure, but because\n",
    "those who do not know how to pursue pleasure\n",
    "rationally encounter consequences that are extremely\n",
    "painful. Nor again is there anyone who loves or\n",
    "pursues or desires to obtain pain of itself, because\n",
    "it is pain, but occasionally circumstances occur in\n",
    "which toil and pain can procure him some great\n",
    "pleasure. To take a trivial example, which of us\n",
    "ever undertakes laborious physical exercise, except\n",
    "to obtain some advantage from it? But who has any\n",
    "right to find fault with a man who chooses to enjoy\n",
    "a pleasure that has no annoying consequences, or\n",
    "one who avoids a pain that produces no resultant\n",
    "pleasure?\n",
    "\n",
    "On the other hand, we denounce with righteous\n",
    "indignation and dislike men who are so beguiled and\n",
    "demoralized by the charms of pleasure of the moment,\n",
    "so blinded by desire, that they cannot foresee the\n",
    "pain and trouble that are bound to ensue; and equal\n",
    "blame belongs to those who fail in their duty\n",
    "through weakness of will, which is the same as\n",
    "saying through shrinking from toil and pain. These\n",
    "cases are perfectly simple and easy to distinguish.\n",
    "In a free hour, when our power of choice is\n",
    "untrammeled and when nothing prevents our being\n",
    "able to do what we like best, every pleasure is to\n",
    "be welcomed and every pain avoided. But in certain\n",
    "circumstances and owing to the claims of duty or\n",
    "the obligations of business it will frequently\n",
    "occur that pleasures have to be repudiated and\n",
    "annoyances accepted. The wise man therefore always\n",
    "holds in these matters to this principle of\n",
    "selection: he rejects pleasures to secure other\n",
    "greater pleasures, or else he endures pains to\n",
    "avoid worse pains.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "exercise_text"
    ]
   },
   "source": [
    "**Exercise 10** (2 points). Let's consider the case when we have more than one \"grocery list\". We want to find the rules which are common to both. \n",
    "\n",
    "Given the following inputs complete the funtion `common_rules`:\n",
    "- `source_0` and `source_1` - strings containing source data. These can be unstructured text, csv, or really anything that can be converted into \"itemsets\". You can assume that both are able to be processed by the same `itemset_maker`.\n",
    "- `itemset_maker` - a function which will process a _single_ data source into itemsets.\n",
    "- `conf_threshold` and `min_count`: Your result should only include rules where `a` occurs in _at least_ `min_count` receipts _and_ $\\mathrm{conf}(a \\Rightarrow b)$ is at least `conf_threshold`.\n",
    "\n",
    "Your function should return a set of tuples, where each tuple is a key in both the rules generated from `source_0` and `source_1`.\n",
    "\n",
    "> Note: We will test your solution using `make_itemsets_unstructured_text` and `make_itemsets_csv` on random inputs. These functions must be correctly defined above to pass the test cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "demo_output_md"
    ]
   },
   "source": [
    "<!-- Expected demo output text block -->\n",
    "The demo included in the solution cell below should display the following output:\n",
    "```\n",
    "{('q', 'u'), ('x', 'e')}\n",
    "```\n",
    "These are the keys for the rules common to both the Latin and English texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0\n",
      "conf(q => u) = 1.000\n",
      "conf(x => e) = 1.000\n",
      "conf(h => i) = 0.833\n",
      "conf(x => i) = 0.833\n",
      "conf(v => t) = 0.818\n",
      "conf(r => e) = 0.800\n",
      "conf(v => e) = 0.773\n",
      "conf(b => i) = 0.750\n",
      "conf(f => i) = 0.750\n",
      "conf(g => i) = 0.750\n"
     ]
    }
   ],
   "source": [
    "def create_rules_from_source(source, itemset_maker, conf_threshold=0, min_count=0):\n",
    "    pair_counts = defaultdict(int)\n",
    "    item_counts = defaultdict(int)\n",
    "\n",
    "    \n",
    "    receipts = itemset_maker(source)\n",
    "\n",
    "\n",
    "  \n",
    "    for receipt in receipts:\n",
    "        update_pair_counts(pair_counts, receipt)\n",
    "        update_item_counts(item_counts, receipt)\n",
    "\n",
    "        \n",
    "\n",
    "    new_item_counts = {}\n",
    "    for key, value in item_counts.items():\n",
    "        if value >= min_count:\n",
    "            new_item_counts[key] = value\n",
    "       \n",
    "    item_counts = new_item_counts\n",
    "\n",
    "\n",
    "    rules = {}\n",
    "    for key, value in pair_counts.items():\n",
    "        if key[0] in item_counts:\n",
    "            rules[key] = value/item_counts[key[0]]\n",
    "\n",
    "\n",
    "    filtered_rules = {}\n",
    "    filtered_rules = filter_rules_by_conf(rules, conf_threshold)\n",
    "    \n",
    "\n",
    "    return filtered_rules\n",
    "    \n",
    "    \n",
    "latin_rules = create_rules_from_source(latin_text, make_itemsets_unstructured_text, 0.75)\n",
    "print('Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0')\n",
    "print_rules(latin_rules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0\n",
      "conf(x => e) = 1.000\n",
      "conf(j => e) = 1.000\n",
      "conf(q => u) = 1.000\n",
      "conf(q => e) = 1.000\n",
      "conf(z => l) = 1.000\n",
      "conf(z => e) = 1.000\n",
      "conf(z => m) = 1.000\n",
      "conf(z => r) = 1.000\n",
      "conf(z => a) = 1.000\n",
      "conf(z => d) = 1.000\n",
      "conf(z => i) = 1.000\n",
      "conf(z => o) = 1.000\n",
      "conf(k => e) = 0.778\n",
      "conf(q => n) = 0.750\n"
     ]
    }
   ],
   "source": [
    "def create_rules_from_source(source, itemset_maker, conf_threshold=0, min_count=0):\n",
    "    pair_counts = defaultdict(int)\n",
    "    item_counts = defaultdict(int)\n",
    "\n",
    "    \n",
    "    receipts = itemset_maker(source)\n",
    "\n",
    "\n",
    "  \n",
    "    for receipt in receipts:\n",
    "        update_pair_counts(pair_counts, receipt)\n",
    "        update_item_counts(item_counts, receipt)\n",
    "\n",
    "        \n",
    "\n",
    "    new_item_counts = {}\n",
    "    for key, value in item_counts.items():\n",
    "        if value >= min_count:\n",
    "            new_item_counts[key] = value\n",
    "       \n",
    "    item_counts = new_item_counts\n",
    "\n",
    "\n",
    "    rules = {}\n",
    "    for key, value in pair_counts.items():\n",
    "        if key[0] in item_counts:\n",
    "            rules[key] = value/item_counts[key[0]]\n",
    "\n",
    "\n",
    "    filtered_rules = {}\n",
    "    filtered_rules = filter_rules_by_conf(rules, conf_threshold)\n",
    "    \n",
    "\n",
    "    return filtered_rules\n",
    "    \n",
    "    \n",
    "english_rules = create_rules_from_source(english_text, make_itemsets_unstructured_text, 0.75)\n",
    "print('Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0')\n",
    "print_rules(english_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('q', 'u'), ('x', 'e')}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('q', 'u'), ('x', 'e')}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def common_rules(source_0, source_1, itemset_maker, conf_threshold, min_count):\n",
    "    pair_counts_one = defaultdict(int)\n",
    "    item_counts_one = defaultdict(int)\n",
    "    pair_counts_two = defaultdict(int)\n",
    "    item_counts_two = defaultdict(int)\n",
    "\n",
    "    words_one = itemset_maker(source_0)\n",
    "    words_two = itemset_maker(source_1)\n",
    "\n",
    "    for word in words_one:\n",
    "        update_pair_counts(pair_counts_one, word) \n",
    "        update_item_counts(item_counts_one, word)  \n",
    "\n",
    "    for word in words_two:\n",
    "        update_pair_counts(pair_counts_two, word)  \n",
    "        update_item_counts(item_counts_two, word)  \n",
    "        \n",
    "    new_item_counts_one = {}\n",
    "    for key, value in item_counts_one.items():\n",
    "        if value >= min_count:\n",
    "            new_item_counts_one[key] = value\n",
    "       \n",
    "    item_counts_one = new_item_counts_one\n",
    "\n",
    "    \n",
    "    new_item_counts_two = {}\n",
    "    for key, value in item_counts_two.items():\n",
    "        if value >= min_count:\n",
    "            new_item_counts_two[key] = value\n",
    "       \n",
    "    item_counts_two = new_item_counts_two\n",
    "  \n",
    "    rules_one = {}\n",
    "    for key, value in pair_counts_one.items():\n",
    "        if key[0] in item_counts_one:\n",
    "            rules_one[key] = value / item_counts_one[key[0]]\n",
    "\n",
    "    rules_two = {}\n",
    "    for key, value in pair_counts_two.items():\n",
    "        if key[0] in item_counts_two:\n",
    "            rules_two[key] = value / item_counts_two[key[0]]\n",
    "\n",
    "    filtered_rules_one = filter_rules_by_conf(rules_one, conf_threshold)  \n",
    "    filtered_rules_two = filter_rules_by_conf(rules_two, conf_threshold)  \n",
    "\n",
    "    k1 = set(filtered_rules_one.keys())\n",
    "    k2 = set(filtered_rules_two.keys())\n",
    "\n",
    "    return k1.intersection(k2)\n",
    "\n",
    "common_rules(latin_text, english_text, make_itemsets_unstructured_text, 0.75, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "test_data_boilerplate"
    ]
   },
   "source": [
    "<!-- Test Cell Boilerplate -->\n",
    "The cell below will test your solution for Exercise 10. The testing variables will be available for debugging under the following names in a dictionary format.\n",
    "- `input_vars` - Input variables for your solution. \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
    "- `returned_output_vars` - Outputs returned by your solution.\n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex10",
     "locked": true,
     "points": "2",
     "solution": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### test_cell_ex10\n",
    "from tester_fw.testers import Tester\n",
    "\n",
    "conf = {\n",
    "    'case_file':'tc_10', \n",
    "    'func': common_rules, # replace this with the function defined above\n",
    "    'inputs':{ # input config dict. keys are parameter names\n",
    "        'source_0':{\n",
    "            'dtype':'str', # data type of param.\n",
    "            'check_modified':False,\n",
    "        },\n",
    "        'source_1':{\n",
    "            'dtype':'str', # data type of param.\n",
    "            'check_modified':False,\n",
    "        },\n",
    "        'itemset_maker':{\n",
    "            'dtype':'function', # data type of param.\n",
    "            'check_modified':False,\n",
    "        },\n",
    "        'conf_threshold':{\n",
    "            'dtype':'float', # data type of param.\n",
    "            'check_modified':False,\n",
    "        },\n",
    "        'min_count':{\n",
    "            'dtype':'str', # data type of param.\n",
    "            'check_modified':False,\n",
    "        }\n",
    "    },\n",
    "    'outputs':{\n",
    "        'output_0':{\n",
    "            'index':0,\n",
    "            'dtype':'set',\n",
    "            'check_dtype': True,\n",
    "            'check_col_dtypes': False, # Ignored if dtype is not df\n",
    "            'check_col_order': False, # Ignored if dtype is not df\n",
    "            'check_row_order': False, # Ignored if dtype is not df\n",
    "            'check_column_type': False, # Ignored if dtype is not df\n",
    "            'float_tolerance': 10 ** (-6)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "tester = Tester(conf, key=b's63L2lglDfBJpcKzxpwcyy61HyKnJNBOJXl9BMyWhyo=', path='resource/asnlib/publicdata/')\n",
    "for _ in range(70):\n",
    "    try:\n",
    "        tester.run_test()\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "    except:\n",
    "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
    "        raise\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": [
     "fin"
    ]
   },
   "source": [
    "**Fin.** If you have made it this far, congratulations on completing the assignment. **Don't forget to submit!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
